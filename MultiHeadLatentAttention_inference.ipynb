{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Rd_IHfv4AQp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RopeLessMLA(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, kv_latent_dim):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.kv_latent_dim = kv_latent_dim\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.dh = self.head_dim\n",
        "\n",
        "        # Projections\n",
        "        self.W_q   = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_dkv = nn.Linear(d_model, kv_latent_dim, bias=False)   # compress to latent\n",
        "        self.W_uk  = nn.Linear(kv_latent_dim, d_model, bias=False)   # decompress keys to model dim\n",
        "        self.W_uv  = nn.Linear(kv_latent_dim, d_model, bias=False)   # <-- FIXED: latent -> model for values\n",
        "        self.W_o   = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.ln = nn.LayerNorm(kv_latent_dim)\n",
        "\n",
        "        # will hold (H, dh, kv_latent_dim) after first forward\n",
        "        self.register_buffer(\"absorbed_k\", None, persistent=False)\n",
        "\n",
        "    def _shape_heads(self, x):  # (B,S,D) -> (B,H,S,dh)\n",
        "        B, S, D = x.shape\n",
        "        return x.view(B, S, self.n_heads, self.dh).permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    def forward(self, x, kv_cache=None, past_length=0):\n",
        "        \"\"\"\n",
        "        x: (B, S, D)\n",
        "        kv_cache: (B, S_cached, kv_latent_dim) or None\n",
        "        \"\"\"\n",
        "        B, S, D = x.size()\n",
        "\n",
        "        # Lazily build absorbed_k = W_q * W_uk  -> shape (D, kv_latent_dim)\n",
        "        # Then split into heads: (H, dh, kv_latent_dim)\n",
        "        if self.absorbed_k is None:\n",
        "            absorbed_k = torch.matmul(self.W_q.weight, self.W_uk.weight)  # (D, kv_latent_dim)\n",
        "            self.absorbed_k = absorbed_k.view(self.n_heads, self.dh, self.kv_latent_dim)\n",
        "\n",
        "        # Build latent KV (and cache)\n",
        "        new_c_kv = self.ln(self.W_dkv(x))  # (B, S, kv)\n",
        "        if kv_cache is None:\n",
        "            c_kv = new_c_kv\n",
        "        else:\n",
        "            c_kv = torch.cat([kv_cache, new_c_kv], dim=1)  # (B, S_full, kv)\n",
        "        S_full = c_kv.size(1)\n",
        "\n",
        "        # Project queries and split to heads\n",
        "        q_full = self.W_q(x)                           # (B, S, D)\n",
        "        q = self._shape_heads(q_full)                  # (B, H, S, dh)\n",
        "\n",
        "        # Precompute (q_h @ absorbed_k_h): for each head h,\n",
        "        # q_h: (B, S, dh), absorbed_k_h: (dh, kv) -> (B, S, kv)\n",
        "        # Then attn_scores_h = (q_h @ absorbed_k_h) @ c_kv^T -> (B, S, S_full)\n",
        "        attn_scores = torch.zeros(B, self.n_heads, S, S_full, device=x.device, dtype=x.dtype)\n",
        "        c_kv_T = c_kv.transpose(1, 2)  # (B, kv, S_full)\n",
        "\n",
        "        for h in range(self.n_heads):\n",
        "            q_h = q[:, h]                                      # (B, S, dh)\n",
        "            absorbed_k_h = self.absorbed_k[h]                  # (dh, kv)\n",
        "            tmp = torch.matmul(q_h, absorbed_k_h)              # (B, S, kv)\n",
        "            attn_scores[:, h] = torch.bmm(tmp, c_kv_T)         # (B, S, S_full)\n",
        "\n",
        "        # Scale + causal mask\n",
        "        attn_scores = attn_scores / (self.dh ** 0.5)\n",
        "        causal = torch.tril(torch.ones(S, S_full, device=x.device), diagonal=past_length)\n",
        "        attn_scores = attn_scores.masked_fill(causal.view(1, 1, S, S_full) == 0, float(\"-inf\"))\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, H, S, S_full)\n",
        "\n",
        "        # Build V: decompress latent to full, then split heads\n",
        "        v_full = self.W_uv(c_kv)                       # (B, S_full, D)  <-- now valid\n",
        "        v = v_full.view(B, S_full, self.n_heads, self.dh).permute(0, 2, 1, 3)  # (B, H, S_full, dh)\n",
        "\n",
        "        # Weighted sum per head -> concat -> output proj\n",
        "        out_heads = []\n",
        "        for h in range(self.n_heads):\n",
        "            context_h = torch.matmul(attn_weights[:, h], v[:, h])  # (B, S, dh)\n",
        "            out_heads.append(context_h)\n",
        "        out = torch.cat(out_heads, dim=-1)  # (B, S, D)\n",
        "\n",
        "        return self.W_o(out), c_kv"
      ],
      "metadata": {
        "id": "ZE18TN5E4iKI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOing mem testing"
      ],
      "metadata": {
        "id": "OHw1VL85GBTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demo():\n",
        "  model= RopeLessMLA(d_model=512,n_heads=8,kv_latent_dim=256)\n",
        "  x=torch.randn(1,5,512)\n",
        "  out,cache=model(x)\n",
        "\n",
        "  print(f\"output : {out.shape}, cache : {cache.shape}\")\n",
        "  std_size=2*2*10*512*4/1024 # standard KV  : B*2(K,V) * T *D * float32\n",
        "  latent_size=2*10*256*4/1024 # KB (Latent Cache : B* T * latent_dim * float32)\n",
        "  print(f\"Memory: Standard={std_size:.2f} KB, Latent={latent_size:.2f} KB\")\n",
        "if __name__==\"__main__\":\n",
        "  demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otO4Wq9cF3mG",
        "outputId": "a7c537e3-03dd-4bb3-8273-5b1933dd3076"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output : torch.Size([1, 5, 512]), cache : torch.Size([1, 5, 256])\n",
            "Memory: Standard=80.00 KB, Latent=20.00 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7DcNaLn5HKRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}